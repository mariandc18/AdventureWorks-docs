{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cab0d55-e963-4407-ada4-55a3695f92d3",
   "metadata": {},
   "source": [
    "# 02.2 Introduction to ETL with PySpark & Minio\n",
    "\n",
    "In this laboratory we will be learning the basics about using PySpark to implement ETL pipelines with a booking database as an example\n",
    "\n",
    "![schema](imgs/bookings-schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c41439-39ab-47df-93dc-8d958776d5e3",
   "metadata": {},
   "source": [
    "## 1. Moving data from local storage to Minio\n",
    "\n",
    "In this task you will need to move the files located in `data/bookings` of the local storage to the Minio bucket named `test`, all the files should have the prefix `lab2/landing/` using Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1660a60-cfc8-47c1-aea1-c0ab4cbdf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_landing_prefix = \"lab2/landing\"\n",
    "local_file_path = \"data/bookings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5e584-6cdf-4876-a0a9-67f7b7367bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure your spark session here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5911192-c814-4f4b-a036-43e18c12ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"bookings\", \"facilities\", \"members\"]\n",
    "\n",
    "for file in files:\n",
    "    # upload the file to Minio\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d3e1f-42ed-4ea9-80f5-804ce5a36fc8",
   "metadata": {},
   "source": [
    "## 2. Transforming the data\n",
    "\n",
    "In this section you will have to read the files you moved from the local storage to a landing zone in the `test` bucket and apply some transformations to them. \n",
    "\n",
    "Then you will save the transformed data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f8bb4-efd3-48ae-bd29-321c50472912",
   "metadata": {},
   "source": [
    "PySpark provides multiple functions to trasnform the data, a majority of these are provided in the `pyspark.sql.functions` module\n",
    "\n",
    "### Column & DataFrame Manipulation\n",
    "\n",
    "| Function   | Purpose                                 | Example                                                        |\n",
    "| ---------- | --------------------------------------- | -------------------------------------------------------------- |\n",
    "| `col`      | Reference a column by name              | `df.select(col(\"age\"))`                                        |\n",
    "| `lit`      | Create a column with a literal value    | `df.withColumn(\"country\", lit(\"USA\"))`                         |\n",
    "| `alias`    | Rename a column in a select             | `df.select(col(\"age\").alias(\"user_age\"))`                      |\n",
    "| `when`     | Conditional expressions (like SQL CASE) | `df.select(when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\"))` |\n",
    "| `coalesce` | Return first non-null value             | `df.select(coalesce(col(\"phone\"), lit(\"N/A\")))`                |\n",
    "\n",
    "\n",
    "### Aggregation & Grouping\n",
    "\n",
    "| Function        | Purpose                  | Example                                 |\n",
    "| --------------- | ------------------------ | --------------------------------------- |\n",
    "| `count`         | Count rows               | `df.groupBy(\"country\").agg(count(\"*\"))` |\n",
    "| `countDistinct` | Count distinct values    | `df.agg(countDistinct(\"user_id\"))`      |\n",
    "| `sum`           | Sum of a column          | `df.agg(sum(\"sales\"))`                  |\n",
    "| `avg` / `mean`  | Average of a column      | `df.agg(avg(\"salary\"))`                 |\n",
    "| `max` / `min`   | Maximum or minimum value | `df.agg(max(\"salary\"), min(\"salary\"))`  |\n",
    "\n",
    "\n",
    "### String Functions\n",
    "| Function         | Purpose                      | Example                                                            |\n",
    "| ---------------- | ---------------------------- | ------------------------------------------------------------------ |\n",
    "| `lower`          | Convert to lowercase         | `df.select(lower(col(\"name\")))`                                    |\n",
    "| `upper`          | Convert to uppercase         | `df.select(upper(col(\"name\")))`                                    |\n",
    "| `concat`         | Concatenate columns          | `df.select(concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))` |\n",
    "| `concat_ws`      | Concatenate with a separator | `df.select(concat_ws(\"-\", \"year\", \"month\", \"day\"))`                |\n",
    "| `substring`      | Extract substring            | `df.select(substring(col(\"phone\"), 1, 3))`                         |\n",
    "| `trim`           | Trim whitespace              | `df.select(trim(col(\"username\")))`                                 |\n",
    "| `regexp_extract` | Extract regex match          | `df.select(regexp_extract(col(\"email\"), r\"@(.+)\", 1))`             |\n",
    "| `regexp_replace` | Replace regex match          | `df.select(regexp_replace(col(\"phone\"), \"-\", \"\"))`                 |\n",
    "\n",
    "\n",
    "### Date & Time Functions\n",
    "\n",
    "| Function                      | Purpose                                | Example                                                               |\n",
    "| ----------------------------- | -------------------------------------- | --------------------------------------------------------------------- |\n",
    "| `current_date`                | Current date                           | `df.select(current_date())`                                           |\n",
    "| `current_timestamp`           | Current timestamp                      | `df.select(current_timestamp())`                                      |\n",
    "| `date_add`                    | Add days to date                       | `df.select(date_add(col(\"start_date\"), 7))`                           |\n",
    "| `date_sub`                    | Subtract days from date                | `df.select(date_sub(col(\"start_date\"), 7))`                           |\n",
    "| `datediff`                    | Difference between two dates (in days) | `df.select(datediff(col(\"end_date\"), col(\"start_date\")))`             |\n",
    "| `months_between`              | Difference in months                   | `df.select(months_between(col(\"end_date\"), col(\"start_date\")))`       |\n",
    "| `year`, `month`, `dayofmonth` | Extract date parts                     | `df.select(year(col(\"date\")), month(col(\"date\")))`                    |\n",
    "| `to_date`                     | Convert string to date                 | `df.select(to_date(col(\"date_string\"), \"yyyy-MM-dd\"))`                |\n",
    "| `to_timestamp`                | Convert string to timestamp            | `df.select(to_timestamp(col(\"datetime_str\"), \"yyyy-MM-dd HH:mm:ss\"))` |\n",
    "\n",
    "\n",
    "### Null handling\n",
    "| Function                           | Purpose                    | Example                               |\n",
    "| ---------------------------------- | -------------------------- | ------------------------------------- |\n",
    "| `isnull`                           | Check for null values      | `df.filter(col(\"email\").isNull())`    |\n",
    "| `isnotnull`                        | Check for non-null values  | `df.filter(col(\"email\").isNotNull())` |\n",
    "| `na.fill` *(method, not function)* | Replace nulls with a value | `df.na.fill({\"email\": \"unknown\"})`    |\n",
    "| `na.drop` *(method, not function)* | Drop rows with nulls       | `df.na.drop()`                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e3d60-eaed-4e86-95d5-b7b76d140026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "transformed_prefix = \"lab2/transformed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac8bc4-2ad8-4f05-bf90-629c615085bb",
   "metadata": {},
   "source": [
    "### 2.1 Transforming members data\n",
    "\n",
    "For members we need to transform the original table so:\n",
    "- Shows the full name of the members\n",
    "- Remove the `surname` and `firstname` columns\n",
    "- The `recommendedby` column should show the full name of the person instead of the id, if the person was not recommended show \"NOT RECOMMENDED\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9bc915-4098-496d-b8a8-b8f87bf7c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark dataframe from the members file uploaded in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2b5df-a827-4d0c-8a05-b42ad6a9afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01040472-ec9b-4134-8c86-f1358c5fa5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see the results with df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f6598-0110-4ad0-89a9-91c2cdecf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to Minio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6474590e-cce3-41c3-b8bf-450620ca6c5b",
   "metadata": {},
   "source": [
    "### 2.2 Transforming the bookings\n",
    "\n",
    "We need a new bookings table that shows\n",
    "\n",
    "- `bookid` : booking id\n",
    "- `memberid`: member id\n",
    "- `member` : the full name of the member that made the booking\n",
    "- `facilityid`: facility id\n",
    "- `facility`: the name of the facility\n",
    "- `starttime`: start time of the booking\n",
    "- `endtime`: end time of the booking\n",
    "- `slots`: amount of hours of the booking\n",
    "- `cost`: the cost of the booking, the per hour cost by the total of hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b678def-1196-4865-a23a-a1749c37aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the dataframes you need from Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508840c0-9921-4756-9eef-db10c4df367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79594fb-1a70-4604-b87e-bbae6e85423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900db492-92c5-47ef-b131-08d1d4018c68",
   "metadata": {},
   "source": [
    "## 3. Reporting \n",
    "\n",
    "You will have to generate reports with the following information:\n",
    "\n",
    "- 3.1 `facility_report_monthly`: For each facility the total bookings, hours booked and revenuee (per month) ordered by facility\n",
    "- 3.2 `facility_report_yearly`: For each facility the total bookings, hours booked and revenue\n",
    "- 3.3 `member_report_monthly`: For each member the total boookings, total hours used, total money spent, and members recommended (per month)\n",
    "- 3.4 `member_report_yearly`: For each member the total hours used and members recommended\n",
    "\n",
    "Only consider the 2012 year\n",
    "\n",
    "You can use any data that you want, save the reports with the prefix `lab2/reports`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1026f1-9449-48fe-b41e-55de700c9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_reports_prefix = \"lab2/reports\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83df8b-3fe8-427a-9415-dfac4b2935eb",
   "metadata": {},
   "source": [
    "Using the bookings report we built in the previous section we already have some of the work done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598fb189-adfb-48ad-834f-048b3afb3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your transformed bookings dataframe here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84ef49-0576-419d-9d63-49abd3e33d41",
   "metadata": {},
   "source": [
    "## 3.1 Facility monthly report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb27a2-7b57-4b4e-978a-bbfcd353e5b5",
   "metadata": {},
   "source": [
    "For breaking down the report into months we will need a rolling dates table, this a table where we will have all the possible dates in a given period of time. In this case we will cosider the entire 2012 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82409092-48af-46c0-a0d1-546598a70915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Generate all the possible dates (remember we only need a month) here ...\n",
    "\n",
    "# dates_df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f22aa-3b50-4c89-a6f9-64486497db6c",
   "metadata": {},
   "source": [
    "## 3.2 Facility yearly report\n",
    "\n",
    "This is quite simple, just an aggregation over the monthly report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b5e0b-974c-4269-8dfc-bd8be66bc963",
   "metadata": {},
   "source": [
    "## 3.3 Members monthly report\n",
    "\n",
    "Like the facility one but with a little twist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d3c62-b5ac-480c-a35b-726c1460b11f",
   "metadata": {},
   "source": [
    "## 3.4 Members yearly report\n",
    "\n",
    "You should be able to do this very easily"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8890ea9-6f07-4c2d-8a23-b92f35aaee67",
   "metadata": {},
   "source": [
    "# Lab 02.1 - Introduction to object storage with S3 & MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b4bca-cda1-4662-bda0-0f5b98261b4e",
   "metadata": {},
   "source": [
    "## What is an object storage\n",
    "\n",
    "An object storage system is a way of storing data as objects in key-value stores (called \"buckets\" or \"containers\"), an object is made of:\n",
    "\n",
    "- **Data**: The actual content of the file (text, image, video, etc.).\n",
    "- **Metadata**: Information about the object, such as file type, creation date, size, permissions, or custom tags you define.\n",
    "\n",
    "Each object is assigned a unique identifier within the bucket, so it can be accessed using the `bucket + key` combination.\n",
    "\n",
    "These stores are then replicated across multiple availability zones and/or regions to ensure high availability and fast access. Users can access these systems over HTTP/HTTPS connections, typically via dedicated APIs. These characteristics make this type of storage ideal and widely used for building data lakes for business intelligence solutions.\n",
    "\n",
    "Among the most popular object stores is [Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html), which adds many extra features over this basic object storage definition and offers a wide range of cheap and reliable storaging options based on the cloud. S3 has set an standard on how these systems work from a functionality standpoint with other storaging services adopting the S3 API conventions for there storage solutions. \n",
    "\n",
    "However, S3 is propietary and paid-by-use software which is a barrier for using it in organizations with resources restrictions, therefore in this course we will be using [MinIO](https://docs.min.io/community/minio-object-store/index.html), which is an open-source and self-hosted alternative to S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264f615-d9f5-4b0d-8be3-005320e95b37",
   "metadata": {},
   "source": [
    "## 1. Connecting to a bucket\n",
    "\n",
    "As mentioned in the previous section object storage is accessed via APIs over an HTTP/HTTPS connection, therefore to connect to a bucket we would need:\n",
    "\n",
    "- `Endpoint`: The URL of the object storage system that manages the bucket\n",
    "- `Bucket`: The bucket identifier\n",
    "- `Key`: A key that identifies the user (think of it as the user name)\n",
    "- `Secret`: A secret that is only known by the user represented by `Key` (think of it as the password)\n",
    "\n",
    "To abstract the intricacies of the API you will tipically use a client to manage the connection, Amazon has made available an SDK (Software Development Kit) for python called [boto3](https://pypi.org/project/boto3/), although there are other clients like [s3fs](https://s3fs.readthedocs.io/en/latest/) used by pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2842b1-baef-4983-a053-cfc8188bb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# We load the configurations of the connection from the environment variables. Never store your credentials in your code files!\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"),\n",
    "    endpoint_url=os.getenv(\"AWS_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2a870-0f9a-4f22-bf47-5e92ef13aeb8",
   "metadata": {},
   "source": [
    "## 2. Upload a file to a bucket\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb829f9-140d-4265-b97e-c940ca6adec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path = \"data/HousePrices.csv\"\n",
    "s3_bucket_name = \"test\"\n",
    "file_prefix = \"notebooks/introduction_to_s3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2fad2-d52a-4bcf-a4cd-226cd4f78938",
   "metadata": {},
   "source": [
    "### 2.1 Upload using boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c8fb5-0a74-485b-b8a5-17ce51dd75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_boto_file_key = f\"{file_prefix}/HousePricesBoto.csv\"\n",
    "s3.upload_file(Filename=local_file_path, Bucket=s3_bucket_name, Key=s3_boto_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d8d6c-b0e0-44e8-9acb-e3770a3f87f6",
   "metadata": {},
   "source": [
    "### 2.2 Upload using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0dd61f-cdfb-49ad-aeb6-adf3146f3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "local_pandas_df = pd.read_csv(local_file_path)\n",
    "\n",
    "s3_pandas_file_key = f\"{file_prefix}/HousePricesPandas.csv\"\n",
    "\n",
    "# All s3 file urls must start with s3:// or with s3a://\n",
    "s3_pandas_file_url = f\"s3a://{s3_bucket_name}/{s3_pandas_file_key}\"\n",
    "\n",
    "local_pandas_df.to_csv(\n",
    "    s3_pandas_file_url,\n",
    "    index=False,\n",
    "    storage_options={ \n",
    "        \"key\" : os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        \"secret\" : os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        \"client_kwargs\" : {\n",
    "            \"endpoint_url\": os.getenv(\"AWS_ENDPOINT\")\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee3908-7151-4e1a-8669-b2d248802c4e",
   "metadata": {},
   "source": [
    "### 2.3 Upload using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631168f-2315-4c26-8d6d-dbd67e316cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkWithS3Files\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", os.getenv(\"AWS_ENDPOINT\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.region\", os.getenv(\"AWS_ENDPOINT\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "load_config(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea72d7-4a8d-40d5-8ee3-0e10d8fc9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_spark_df = spark.read.csv(\n",
    "    local_file_path,   \n",
    "    header=True,         \n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Spark by default will create a folder and within it will place the result file in chunks (worker node chunks)\n",
    "s3_spark_file_key = f\"{file_prefix}/HousePricesSpark\"\n",
    "\n",
    "# All s3 file urls must start with s3:// or with s3a://\n",
    "s3_spark_file_url = f\"s3a://{s3_bucket_name}/{s3_spark_file_key}\"\n",
    "\n",
    "local_spark_df.write\\\n",
    "    .format('csv')\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(s3_spark_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de7b67-9350-4775-8bf5-c55a2626934a",
   "metadata": {},
   "source": [
    "## 3. Read a file from a bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32675ed1-482e-49ea-a004-d41b9d2483a3",
   "metadata": {},
   "source": [
    "### 3.1 Read using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a297ec-66f5-45ed-92be-ed894b465de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "response = s3.get_object(Bucket=s3_bucket_name, Key=s3_boto_file_key)\n",
    "\n",
    "\n",
    "with open(local_file_path, \"rb\") as f:\n",
    "    local_file_data = f.read()\n",
    "\n",
    "s3_file_data = response[\"Body\"].read()\n",
    "\n",
    "# check that the hash of the local file and the s3 file matches\n",
    "assert hashlib.sha256(local_file_data).hexdigest() == hashlib.sha256(s3_file_data).hexdigest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553e212-4712-4c3a-871a-de90891deca9",
   "metadata": {},
   "source": [
    "### 3.1 Read using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252850c-6431-4282-a974-98e26f2f2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_pandas_df = pd.read_csv(\n",
    "    s3_pandas_file_url, \n",
    "    storage_options={ \n",
    "        \"key\" : os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        \"secret\" : os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        #\"region\" : os.getenv(\"AWS_REGION\"),\n",
    "        \"client_kwargs\" : {'endpoint_url': os.getenv(\"AWS_ENDPOINT\")},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check that the dataframe was saved correctly\n",
    "assert len(local_pandas_df.columns) == len(s3_pandas_df.columns)\n",
    "assert len(local_pandas_df) == len(s3_pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b914e34-0de6-4ad0-87aa-74d795faf4b6",
   "metadata": {},
   "source": [
    "### 3.3 Read using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2988225-c1a1-455a-85d7-07a3a86c4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_spark_df = spark.read.csv(\n",
    "    s3_spark_file_url,\n",
    "    header=True,      # Use first row as column names\n",
    "    inferSchema=True  # Automatically detect data types\n",
    ")\n",
    "\n",
    "assert local_spark_df.count() == s3_spark_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

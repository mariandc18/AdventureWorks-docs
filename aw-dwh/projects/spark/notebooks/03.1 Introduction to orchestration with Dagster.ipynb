{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4432dfb6-60c0-4ca9-83ac-35d63e22bed7",
   "metadata": {},
   "source": [
    "# 03.1 Introduction to orchestration with Dagster\n",
    "\n",
    "## The Dagster orchestration model\n",
    "\n",
    "\n",
    "Dagster is a data orchestration framework that manages the flow of data and computation across pipelines (called jobs) made up of reusable components (either assets or ops).\n",
    "\n",
    "Unlike the schedulers that consider each element of the graph a task (like Airflow), Dagster has different types of nodes that describe how data is produced and cosumed. [Source](https://docs.dagster.io/getting-started/concepts)\n",
    "\n",
    "The main types we are going to be using in this lesson are:\n",
    "\n",
    "- [Assets](https://docs.dagster.io/guides/build/assets): An asset is an object in persistent storage, such as a table, file, or persisted machine learning model. An asset definition is a description, in code, of an asset that should exist and how to produce and update that asset. \n",
    "\n",
    "- [Resources](https://docs.dagster.io/guides/build/external-resources): Dagster resources are objects used by Dagster assets and ops that provide access to external systems, databases, or services. For example, a simple ETL (Extract Transform Load) pipeline fetches data from an API, ingests it into a database, and updates a dashboard.\n",
    "\n",
    "- [IO Managers](https://docs.dagster.io/guides/build/io-managers): I/O managers in Dagster allow you to keep the code for data processing separate from the code for reading and writing data.  These are considered a special type of resource.\n",
    "\n",
    "- [Jobs](https://docs.dagster.io/guides/build/jobs): Jobs are the main unit of execution and monitoring in Dagster. They allow you to execute a portion of a graph of asset definitions or ops based on a schedule or an external trigger.\n",
    "\n",
    "- [Schedule](https://docs.dagster.io/guides/automate/schedules): Schedules enable automated execution of jobs at specified intervals. These intervals can range from common frequencies like hourly, daily, or weekly, to more complex patterns defined using cron expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850f5d8-0538-441d-afc2-43b06db6e635",
   "metadata": {},
   "source": [
    "## Project structure\n",
    "\n",
    "The dagster project that we are going to build during this course is located at `aw-dwh/projects/dagster/adventureworks-orchestration`, you can find more information about how to create a new project in the [official documentation](https://docs.dagster.io/guides/build/projects/creating-a-new-project).\n",
    "\n",
    "The project is structured as follows\n",
    "\n",
    "\n",
    "```\n",
    "adventureworks_orchestration/\n",
    "┣ assets/ <- define the assets here\n",
    "┣ resources/ <- define the resources here\n",
    "┣ __init__.py\n",
    "┣ constants.py\n",
    "┣ definitions.py <- load definitions from assets, resources, jobs, etc..\n",
    "┗ jobs.py <- define the jobs here\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a36c5-da4a-4489-92ac-02df479668f8",
   "metadata": {},
   "source": [
    "## Defining resources and IO managers\n",
    "\n",
    "As stated before Dagster resources are an abstraction over external services that are used to construct assets, while IO managers handle how these assets are read and written to external storage\n",
    "\n",
    "In this case, since we are going to be automating the code from the previous lesson we only need a Spark session with access to S3.\n",
    "\n",
    "Luckily for us, dagster has handful of pre-made resources that we can use, including a PySpark one, we just need to configure it to access the our S3 compatible storage (MinIO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a395c6-e7ac-4286-8e34-85c2954dc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions.py\n",
    "\n",
    "from dagster_pyspark import pyspark_resource\n",
    "import os\n",
    "\n",
    "\n",
    "@definitions\n",
    "def defs():\n",
    "\n",
    "    configured_pyspark = pyspark_resource.configured(\n",
    "        {\n",
    "            \"spark_conf\": {\n",
    "                \"spark.hadoop.fs.s3.impl\": \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\",\n",
    "                \"fs.s3a.access.key\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "                \"fs.s3a.secret.key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "                \"fs.s3a.endpoint\": os.getenv(\"AWS_ENDPOINT\"),\n",
    "                \"fs.s3a.region\": os.getenv(\"AWS_REGION\")\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return Definitions(\n",
    "        # other code...\n",
    "        resources={\n",
    "            \"spark_s3_rsc\" : configured_pyspark\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188af4b-f66e-464d-9f3a-58b1066b3302",
   "metadata": {},
   "source": [
    "Then we can use this configured PySpark session to handle the IO of our assets, in this case we are going to be saving our assets to CSV files so we can easily check.\n",
    "\n",
    "To define our custom resource we can create a `csv_io_manager` inside the `resources` folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230ff90-01d4-4281-9e3c-6e864692be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resources/csv_io_manager.py\n",
    "\n",
    "import os\n",
    "from typing import Union\n",
    "import dagster as dg\n",
    "from dagster_pyspark.resources import PySparkResource\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "class PartitionedCsvIOManager(dg.ConfigurableIOManager):\n",
    "    \n",
    "    pyspark: dg.ResourceDependency[PySparkResource]\n",
    "\n",
    "    @property\n",
    "    def _base_path(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def handle_output(self, context: dg.OutputContext, obj: DataFrame):\n",
    "        path = self._get_path(context)\n",
    "\n",
    "        # code to save the asset to external storage here...\n",
    "\n",
    "        # add metadata to the asset here...\n",
    "        context.add_output_metadata({\"path\": path})\n",
    "\n",
    "\n",
    "    def load_input(self, context) -> DataFrame:\n",
    "        path = self._get_path(context)\n",
    "\n",
    "        # code to read the asset from external storage here...\n",
    "\n",
    "    \n",
    "    def _get_path(self, context: Union[dg.InputContext, dg.OutputContext]):\n",
    "        return os.path.join(self._base_path, *context.asset_key.parts)\n",
    "\n",
    "\n",
    "class S3PartitionedCsvIOManager(PartitionedCsvIOManager):\n",
    "    s3_bucket: str\n",
    "\n",
    "    @property\n",
    "    def _base_path(self):\n",
    "        return \"s3a://\" + self.s3_bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13451d7c-7e93-423e-9888-a3409c30ee97",
   "metadata": {},
   "source": [
    "Then we can also register this IO manager in our definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af4ff0-8268-473f-98c9-e0f8767bbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions.py\n",
    "\n",
    "return Definitions(\n",
    "    assets=[*sample_assets],\n",
    "    jobs=jobs,\n",
    "    resources={\n",
    "        \n",
    "        \"spark_s3_rsc\" : configured_pyspark,\n",
    "        \"s3_test_io_manager\": S3PartitionedCsvIOManager(\n",
    "            pyspark=configured_pyspark,\n",
    "            s3_bucket=\"test\"\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d51b0-a8d5-4336-bbbf-b4ef8fe2afce",
   "metadata": {},
   "source": [
    "## Defining the assets\n",
    "\n",
    "The assets in Dagster represents data in the persistent storage, from the last lesson we had local files, then we uploaded those files, and then we transformed those and created reports from the transformed data.\n",
    "\n",
    "\n",
    "So the dependencies between our assets should look something like this:\n",
    "\n",
    "[Source] -> [Transformed] -> [Reports]\n",
    "\n",
    "Lets start with the source assets. In this lesson we will be creating a `sample` folder inside the `assets` folder, to group all the assets for this lesson, this is a recommended practice, please note that the folder should be a python module (has a `__init__.py` file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239294e-0cf5-4a6f-99a2-5303cf88dacd",
   "metadata": {},
   "source": [
    "### Our first assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09145df-1115-42ed-813a-4e7e770e16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assets/sample/assets.py\n",
    "\n",
    "import dagster as dg\n",
    "from adventureworks_orchestration.constants import ASSET_GROUP_LABS\n",
    "from .constants import ASSET_PREFIX_SOURCE, ASSET_PREFIX_TRANSFORMED, ASSET_PREFIX_REPORTS\n",
    "from dagster_pyspark import PySparkResource\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "files_path = \"/home/iceberg/notebooks/data/bookings\"\n",
    "files = [\"bookings\", \"facilities\", \"members\"]\n",
    "\n",
    "\n",
    "@dg.multi_asset(\n",
    "    outs={\n",
    "        file: dg.AssetOut(\n",
    "            io_manager_key=\"s3_test_io_manager\",\n",
    "            key=[ASSET_GROUP_LABS, ASSET_PREFIX_SOURCE, file],\n",
    "            group_name=ASSET_GROUP_LABS,\n",
    "            is_required=False\n",
    "        ) for file in files\n",
    "    },\n",
    "    can_subset=True\n",
    ")\n",
    "def bookings_source_files(context: dg.AssetExecutionContext, spark_s3_rsc: PySparkResource):\n",
    "    spark = spark_s3_rsc.spark_session\n",
    "\n",
    "    has_errors = False\n",
    "    \n",
    "    for file in context.selected_output_names:\n",
    "        try:\n",
    "\n",
    "            # read the file from local storage using PySpark\n",
    "            # df = ?\n",
    "\n",
    "            context.log.info(f\"Finshed reading file {file}.csv\")\n",
    "\n",
    "            yield dg.Output(\n",
    "                output_name=file,\n",
    "                value=df\n",
    "            )\n",
    "\n",
    "        except Exception:\n",
    "            context.log.exception(f\"Couldn't materialize the asset {file}\")\n",
    "            has_errors = True\n",
    "\n",
    "    if has_errors:\n",
    "        raise Exception(\"Errors while materializing the assets\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb4ad8-206b-4d71-a82c-a597245ec70b",
   "metadata": {},
   "source": [
    "Lets break down the code\n",
    "\n",
    "The `multi_asset` decorator indicates that the method `bookings_source_files` produces multiple assets\n",
    "\n",
    "The `outs` parameters is a dictionary for specifying the properties of each one of the assets the method produces, in this case all assets will be saved to CSV files so we use the same `io_manager_key` which refers to our custom CSV IO manager implemented early. The `key` of the asset is custom string, in this case a string made of different parts `ASSET_GROUP_LABS`, `ASSET_PREFIX_SOURCE` and the file name; this way we make sure the naming of assets stay consistent with the format `labs/source/{file}`. The `group_name` parameter is to group assets, which comes handy when defining jobs later. The `is_required` indicates if method should raise an error if the asset is not returned.\n",
    "\n",
    "We also passed a `can_subset=True` which for a multi asset means that the method can be used to materialize only a subset of the assets.\n",
    "\n",
    "In the body of the method we got the spark session from the PySparkResource we defined earlier. Then for each one of the assets we need to materialize we need to read them from the local storage. Once the file is read into a DataFrame we can yield is value using the `Output` class. Once Dagster receives the `Output` object will pass it to the IO manager we defined which will save the asset to MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b35d2b-ee38-4117-8241-9e14ba0d6316",
   "metadata": {},
   "source": [
    "### Dependencies between assets\n",
    "\n",
    "We can declare dependencies between assets using the `ins` parameter in the `asset` decorator, we just need to specify the key of the asset and the IO manager will automatically retrieve it from the storage for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232bfdd6-ce21-4f92-8cd8-6e951cdb0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assets/sample/assets.py\n",
    "\n",
    "@dg.asset(\n",
    "    key=[ASSET_GROUP_LABS, ASSET_PREFIX_TRANSFORMED, \"members\"],\n",
    "    group_name=ASSET_GROUP_LABS,\n",
    "    ins={\n",
    "        \"members\": dg.AssetIn(key=[ASSET_GROUP_LABS, ASSET_PREFIX_SOURCE,\"members\"])\n",
    "    },\n",
    "    io_manager_key=\"s3_test_io_manager\"\n",
    ")\n",
    "def bookings_transformed_members(context: dg.AssetExecutionContext, members: DataFrame):\n",
    "\n",
    "    # transform the members dataframe ...\n",
    "    \n",
    "    return dg.Output(value=result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b1ccf-4947-4cc3-988c-f0ccbbcafab8",
   "metadata": {},
   "source": [
    "### Registering the assets\n",
    "\n",
    "We need to register our assets in the definition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd6b2a-b16f-4b72-b426-50c4726240cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dagster as dg\n",
    "from .assets import sample\n",
    "#...\n",
    "\n",
    "@dg.definitions\n",
    "def defs():\n",
    "    #...\n",
    "\n",
    "    sample_assets = dg.load_assets_from_package_module(sample) # load assets from the module\n",
    "    \n",
    "    return dg.Definitions(\n",
    "        assets=[*sample_assets],\n",
    "        resources={\n",
    "            \"spark_s3_rsc\" : configured_pyspark,\n",
    "            \"s3_test_io_manager\": S3PartitionedCsvIOManager(\n",
    "                pyspark=configured_pyspark,\n",
    "                s3_bucket=\"test\"\n",
    "            )\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d1850-147c-4a66-8c6a-471943909fc2",
   "metadata": {},
   "source": [
    "## Defining a job\n",
    "\n",
    "A job definition is very easy to do, you just need a name and select the assets that you want to materialize in the job. In this case we are selecting all the assets in the group `ASSET_GROUP_LABS` we defined at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb848850-0889-4985-8914-956c83363462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs.py\n",
    "\n",
    "sample_job = dg.define_asset_job(\n",
    "        \"sample_job\",\n",
    "        selection=AssetSelection.groups(ASSET_GROUP_LABS)\n",
    "    )\n",
    "\n",
    "jobs = [sample_job]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0af0b8-7c85-41f5-8358-0f4230584ea6",
   "metadata": {},
   "source": [
    "Then we register these jobs in our definitions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862096c-8024-4bdd-b04a-7b42bc6eaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .jobs import jobs\n",
    "\n",
    "#...\n",
    "\n",
    "@dg.efinitions\n",
    "def defs():\n",
    "    #...\n",
    "    \n",
    "    return dg.Definitions(\n",
    "        assets=[*sample_assets],\n",
    "        jobs=jobs,\n",
    "        resources={\n",
    "            \"spark_s3_rsc\" : configured_pyspark,\n",
    "            \"s3_test_io_manager\": S3PartitionedCsvIOManager(\n",
    "                pyspark=configured_pyspark,\n",
    "                s3_bucket=\"test\"\n",
    "            )\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a95f37-d1fa-4a02-a1b1-9ef9bf3631cf",
   "metadata": {},
   "source": [
    "## Scheduling\n",
    "\n",
    "Creating a schedule is very easy with the `ScheduleDefinition` class, you just need to define the job you want to schedule and the cron expression that should trigger it. Then you will need to register the schedule in your definitions file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a565bc-86ed-4c71-8c8c-43131e9efcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_minutes_schedule = dg.ScheduleDefinition(\n",
    "    job=sample_job,\n",
    "    cron_schedule=\"*/5 * * * *\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664cd77-0a8f-4f22-9696-a9ad75c4f9af",
   "metadata": {},
   "source": [
    "## Using Dagster server to run your pipeline\n",
    "\n",
    "If your dagster code is fine you should be able to see it in the Dagster manager \"Definitions\" tab with a `Loaded` status\n",
    "\n",
    "![](./imgs/dagster/definitions.png)\n",
    "\n",
    "From the manager you can also see your assets as well as their dependency lineage from the \"Assets\" tab \n",
    "\n",
    "![](./imgs/dagster/assets.png)\n",
    "\n",
    "![](./imgs/dagster/lineage.png)\n",
    "\n",
    "You can also control the jobs from the \"Jobs\" tab\n",
    "\n",
    "![](./imgs/dagster/jobs.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
